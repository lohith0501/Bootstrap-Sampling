{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-work - Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import sqrt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, confusion_matrix, auc, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "#set visual parameters\n",
    "%matplotlib inline\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 14,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for printing in bold style\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Petrol_tax</th>\n",
       "      <th>Average_income</th>\n",
       "      <th>Paved_Highways</th>\n",
       "      <th>Population_Driver_licence(%)</th>\n",
       "      <th>Petrol_Consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3571</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.525</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4092</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.572</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Petrol_tax  Average_income  Paved_Highways  Population_Driver_licence(%)  \\\n",
       "0         9.0            3571            1976                         0.525   \n",
       "1         9.0            4092            1250                         0.572   \n",
       "\n",
       "   Petrol_Consumption  \n",
       "0                 541  \n",
       "1                 524  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petrol = pd.read_csv(\"petrol_consumption.csv\")\n",
    "petrol.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48 entries, 0 to 47\n",
      "Data columns (total 5 columns):\n",
      "Petrol_tax                      48 non-null float64\n",
      "Average_income                  48 non-null int64\n",
      "Paved_Highways                  48 non-null int64\n",
      "Population_Driver_licence(%)    48 non-null float64\n",
      "Petrol_Consumption              48 non-null int64\n",
      "dtypes: float64(2), int64(3)\n",
      "memory usage: 2.0 KB\n"
     ]
    }
   ],
   "source": [
    "petrol.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This dataset is a small and clean dataset without any missing values.We can now separate the target variable and split the data for train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the dependent and independent variables.\n",
    "target = petrol['Petrol_Consumption']\n",
    "train = petrol.copy()\n",
    "train.drop('Petrol_Consumption', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(train, target, test_size = 0.2, random_state = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets HOLD out the TEST dataset for final predictions. And by using bootstrapping, lets generate random sample splits on the TRAIN dataset.We will NOT use the TEST dataset for tuning our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = train_x.shape[0] #the number of rows of train dataset.\n",
    "all_indices = list(range(nrows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BootStrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will train our initial Base Model using Bootstrapping Sampling technique\n",
    "Note: Why Bootstrapping? Since this is a smaller dataset, KFold doesnot hold good. We can consider a sampling technique with replacement. Hence we will use BootStrapping resampling technique. LOOCV can be another option.\n",
    "STEP 1: Firstly, using bootstrapping we will generate new TRAIN set and VALIDATION set.\n",
    "STEP 2: We will iterate the resampling technique for n=10 number of times to generate random train and validation sets.\n",
    "STEP 3: For each iteration, we will predict the labels on VALIDATION set and calculate the evaluation metrics(in this case-rmse, r2, mae).\n",
    "STEP 4: We will calculate the mean of all evaluation metrics to get the overall scores of our base model.\n",
    "STEP 5: Finally, we will use the TEST set, only once, to evaluate the base model on unseen observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_boot = DecisionTreeRegressor(random_state=123)\n",
    "mean_rmse = 0.0\n",
    "mean_r2 = 0.0\n",
    "mean_mae = 0.0\n",
    "n = 10 #running the bootstrap for 10 iterations.\n",
    "for i in range(n):\n",
    "    #for each iteration, randomly generate new train and validation set.\n",
    "    train_indices = resample(all_indices, replace=True, n_samples=int(0.7*nrows), random_state=i*32)\n",
    "    validation_indices = [index for index in list(range(nrows)) if index not in train_indices]\n",
    "    \n",
    "    new_train_x = train_x.take(train_indices) #get the new train_x samples\n",
    "    validation_x = train_x.take(validation_indices) #get the new validation_x samples\n",
    "    \n",
    "    new_train_y = train_y.take(train_indices)\n",
    "    validation_y = train_y.take(validation_indices)\n",
    "    \n",
    "    dtree_boot.fit(new_train_x, new_train_y) #fit the model on the new train set\n",
    "    \n",
    "    preds_boot = dtree_boot.predict(validation_x)#predict the labels on the validation set\n",
    "    rmse_boot = np.sqrt(mean_squared_error(validation_y, preds_boot))\n",
    "    r2_boot = r2_score(validation_y, preds_boot)\n",
    "    mae_boot = mean_absolute_error(validation_y, preds_boot)\n",
    "\n",
    "    mean_rmse += rmse_boot\n",
    "    mean_r2 += r2_boot\n",
    "    mean_mae += mae_boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we got the evaulation metrics from 10 iterations of validation sets, lets calculate the mean to get an overall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Evaluation Metrics on Validation Set:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE:145.74540117052433  | Mean R2 Score:-0.5529971779064724  |  Mean MAE:110.81994754041969\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Evaluation Metrics on Test Set:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:44.74371464239419  | Test R2 Score:0.4897230944904368  |  Test MAE:34.4\n"
     ]
    }
   ],
   "source": [
    "mean_rmse = mean_rmse/n\n",
    "mean_r2 = mean_r2/n\n",
    "mean_mae = mean_mae/n\n",
    "printmd('**Evaluation Metrics on Validation Set:**')\n",
    "print(\"Mean RMSE:{}  | Mean R2 Score:{}  |  Mean MAE:{}\".format(mean_rmse,mean_r2, mean_mae))\n",
    "\n",
    "#predict on the test set for base test evaluation metrics.\n",
    "preds_base = dtree_boot.predict(test_x)\n",
    "rmse_base = np.sqrt(mean_squared_error(test_y, preds_base))\n",
    "r2_base = r2_score(test_y, preds_base)\n",
    "mae_base = mean_absolute_error(test_y, preds_base)\n",
    "\n",
    "printmd('**Evaluation Metrics on Test Set:**')\n",
    "print(\"Test RMSE:{}  | Test R2 Score:{}  |  Test MAE:{}\".format(rmse_base,r2_base, mae_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### une the base model by tweaking hyperparameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Best Parameters After Tuning:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'mse', 'max_depth': 2, 'max_features': 2, 'max_leaf_nodes': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "#define list of parameters for the the DecisionTreeRegressor\n",
    "param_grid = {'criterion' : ['mse', 'mae'],\n",
    "              'max_depth': [2,4,6,8,None],\n",
    "              'min_samples_leaf': [0.5, 1, 2],\n",
    "              'min_samples_split':[2, 3, 4, 6],\n",
    "              'max_leaf_nodes' : [None,2, 4, 6],\n",
    "              'max_features': [2,3,4]}\n",
    "\n",
    "#define a tuned_tree using GridSearchCV with the above set param_grid and with different set of folds.\n",
    "dtree_gs = GridSearchCV(DecisionTreeRegressor(random_state=123), param_grid=param_grid, \n",
    "                           scoring = 'neg_mean_absolute_error', cv =10)#Using scoring method as 'neg_mean_absolute_error', model aims at reducing the MAE. \n",
    "#We can also try with other scoring methods.\n",
    "dtree_gs.fit(train_x,train_y)\n",
    "\n",
    "printmd(\"**Best Parameters After Tuning:**\")\n",
    "print(dtree_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Best Parameters from GridSearchCV, we will train a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Base Evaluation Metrics on Validation Set:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE:145.74540117052433  | Mean R2 Score:-0.5529971779064724  |  Mean MAE:110.81994754041969\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Base Evaluation Metrics on Test Set:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:44.74371464239419  | Test R2 Score:0.4897230944904368  |  Test MAE:34.4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Final Evaluation Metrics on Test Set:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:36.83345514635533  | Test R2 Score:0.6541985904891294  |  Test MAE:28.088888888888892\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Parameters Before Tuning:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'mse', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': 123, 'splitter': 'best'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Parameters After Tuning:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'mse', 'max_depth': 2, 'max_features': 2, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': 123, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "dtree_final = DecisionTreeRegressor(criterion = 'mse', max_depth = 2, max_features = 2,\n",
    "                                    min_samples_leaf = 2, min_samples_split = 2, random_state=123)\n",
    "dtree_final.fit(train_x, train_y)\n",
    "\n",
    "#Final Test Predictions\n",
    "preds_final = dtree_final.predict(test_x)\n",
    "rmse_final = np.sqrt(mean_squared_error(test_y, preds_final))\n",
    "r2_final = r2_score(test_y, preds_final)\n",
    "mae_final = mean_absolute_error(test_y, preds_final)\n",
    "\n",
    "#print base evaluation metrics\n",
    "printmd('**Base Evaluation Metrics on Validation Set:**')\n",
    "print(\"Mean RMSE:{}  | Mean R2 Score:{}  |  Mean MAE:{}\".format(mean_rmse,mean_r2, mean_mae))\n",
    "printmd('**Base Evaluation Metrics on Test Set:**')\n",
    "print(\"Test RMSE:{}  | Test R2 Score:{}  |  Test MAE:{}\".format(rmse_base,r2_base, mae_base))\n",
    "printmd('**Final Evaluation Metrics on Test Set:**')\n",
    "print(\"Test RMSE:{}  | Test R2 Score:{}  |  Test MAE:{}\".format(rmse_final,r2_final, mae_final))\n",
    "\n",
    "printmd(\"**Parameters Before Tuning:**\")\n",
    "print(dtree_boot.get_params())\n",
    "printmd(\"**Parameters After Tuning:**\")\n",
    "print(dtree_final.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------INFERENCE------------------------------------------\n",
    "we can conclude that with the best parameters,\n",
    "TEST Evaluation metrics have further improved significantly.\n",
    "R2 Score improved from 0.48 to 0.65\n",
    "RMSE reduced from 44.74 to 36.83\n",
    "MAE reduced from 34.4 to 28.08.\n",
    "We can also play around with more parameters to get a fine tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
